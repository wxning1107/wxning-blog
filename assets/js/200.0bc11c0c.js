(window.webpackJsonp=window.webpackJsonp||[]).push([[200],{861:function(t,n,v){"use strict";v.r(n);var _=v(15),s=Object(_.a)({},(function(){var t=this,n=t.$createElement,v=t._self._c||n;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"并行排序"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#并行排序"}},[t._v("#")]),t._v(" 并行排序")]),t._v(" "),v("p",[t._v("算法的目的就是为了提高代码执行的效率。那"),v("strong",[t._v("当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢")]),t._v("？")]),t._v(" "),v("p",[t._v("一个非常简单又好用的优化方法，那就是并行计算。"),v("strong",[t._v("如何利用并行处理提高算法的执行效率？")])]),t._v(" "),v("p",[t._v("假设我们要给大小为 8GB 的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为 O(nlogn) 的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题，已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给 8GB 数据排序问题的执行效率提高很多倍。具体的实现思路有下面两种。")]),t._v(" "),v("p",[v("strong",[t._v("第一种是对归并排序并行化处理")]),t._v("。我们可以将这 8GB 的数据划分成 16 个小的数据集合，每个集合包含 500MB 的数据。我们用 16 个线程，并行地对这 16 个 500MB 的数据集合进行排序。这 16 个小集合分别排序完成之后，我们再将这 16 个有序集合合并。")]),t._v(" "),v("p",[v("strong",[t._v("第二种是对快速排序并行化处理")]),t._v("。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成 16 个小区间。我们将 8GB 的数据划分到对应的区间中。针对这 16 个小区间的数据，我们启动 16 个线程，并行地进行排序。等到 16 个线程都执行结束之后，得到的数据就是有序数据了。")]),t._v(" "),v("p",[t._v("对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。")]),t._v(" "),v("p",[t._v("这里我还要多说几句，如果要排序的数据规模不是 8GB，而是 1TB，那问题的重点就不是算法的执行效率了，而是数据的读取效率。因为 1TB 的数据肯定是存在硬盘中，无法一次性读取到内存中，这样在排序的过程中，就会有频繁地磁盘数据的读取和写入。如何减少磁盘的 IO 操作，减少磁盘数据读取和写入的总量，就变成了优化的重点。不过这个不是我们这节要讨论的重点，你可以自己思考下。")])])}),[],!1,null,null,null);n.default=s.exports}}]);